> :speech_balloon: Deixe o seu feedback sobre este roteiro no rodap√©. [^1] 
![](https://eni.bb.com.br/eni1/matomo.php?idsite=469&amp;rec=1&amp;url=https://fontes.intranet.bb.com.br/dev/publico/roteiros/-/blob/master/kubernetes/referencias/alocar_recursos_microsservicos.md&amp;action_name=kubernetes/referencias/alocar_recursos_microsservicos)

# Aloca√ß√£o de recursos de CPU e Mem√≥ria para os Microsservi√ßos

Este roteiro apresenta conceitos fundamentais e traz um caso de uso sobre como alocar recursos de CPU e mem√≥ria para microsservi√ßos de forma eficiente. O objetivo √© ajudar o desenvolvedor a configurar corretamente esses recursos, controlando custos sem comprometer o desempenho das aplica√ß√µes.

## Conceitos fundamentais 

|Conceito|Defini√ß√£o|Observa√ß√£o|
|--|--|--|
|**Requests**| Configura√ß√£o do YAML, determina a quantidade m√≠nima de recursos (CPU e/ou mem√≥ria) que deve ser alocada para cada cont√™iner. Sem ela, o Pod n√£o ser√° inicializado.| √â crucial definir corretamente os requests para evitar subaloca√ß√£o ou superaloca√ß√£o de recursos, garantindo que o pod sempre tenha o m√≠nimo necess√°rio para funcionar.|
|**Limits**| Configura√ß√£o do YAML, determina a quantidade m√°xima de recursos (CPU e/ou mem√≥ria) que cada cont√™iner pode utilizar. No entanto, n√£o √© garantido que o cont√™iner conseguir√° essa quantidade de recursos.| Para CPU, o ideal √© que o **limits seja pelo menos 6 vezes maior que o request**, pois h√° um pico de processamento durante a inicializa√ß√£o do pod.|
|**Recursos aloc√°veis**| A quantidade de recursos dispon√≠veis para serem alocados aos cont√™ineres, ap√≥s subtrair os recursos usados pelo sistema e outros servi√ßos.| O conhecimento exato dos recursos aloc√°veis √© vital para evitar a sobrecarga do n√≥ e garantir a efici√™ncia na aloca√ß√£o de recursos. |
|**Recursos compress√≠veis**| Recursos que podem ser ajustados ou limitados sem causar uma falha imediata no funcionamento do cont√™iner ou aplica√ß√£o. O desempenho pode ser degradado, mas o sistema continua funcionando.| A CPU √© o exemplo mais comum de um recurso compress√≠vel. |
|**Recursos incompress√≠veis**| Recursos que n√£o podem ser limitados ou ajustados sem causar falhas imediatas ou problemas cr√≠ticos no funcionamento do cont√™iner ou aplica√ß√£o. |A mem√≥ria √© o exemplo mais comum de um recurso incompress√≠vel. A configura√ß√£o correta da mem√≥ria √© essencial para evitar OOM (Out Of Memory), que pode causar falhas cr√≠ticas na aplica√ß√£o.|
|**Throttling**| Redu√ß√£o da capacidade de CPU de um pod para n√£o exceder os limites configurados. √â utilizado para garantir que os recursos sejam distribu√≠dos de forma justa entre os pods, evitando que um √∫nico pod monopolize a CPU.| Por exemplo, se um n√≥ tem 4 CPUs virtuais (vCPUs) e cada pod est√° configurado para usar no m√°ximo 1 vCPU, o Kubernetes ajustar√° dinamicamente o uso de CPU para garantir que cada pod n√£o ultrapasse sua aloca√ß√£o, distribuindo eficientemente os recursos dispon√≠veis.|
|**Eviction**| Remo√ß√£o de um pod do n√≥ devido a falta de recursos cr√≠ticos, como mem√≥ria (mais comum para recursos incompress√≠veis). A pol√≠tica de eviction deve ser bem compreendida para evitar a interrup√ß√£o de servi√ßos cr√≠ticos. | Por exemplo, um n√≥ tem 16 GB de mem√≥ria e 7 pods, com cada pod limitado a 2 GB. Se um novo pod precisar de 4 GB, n√£o haver√° mem√≥ria suficiente dispon√≠vel. Nesse cen√°rio, o Kubernetes remover√° um pod existente para liberar espa√ßo.|
|**Cgroups (Control Groups)**| Mecanismo do kernel Linux que permite a limita√ß√£o, contabiliza√ß√£o e isolamento de recursos de um grupo de processos. Os grupos s√£o usados para controlar e monitorar recursos como CPU e mem√≥ria.| - |
|**CPU Shares**| As shares definem prioridades relativas de CPU entre diferentes Cgroups. Grupos com mais shares t√™m acesso proporcionalmente maior √† CPU quando houver competi√ß√£o por recursos.| Imagine dois Cgroups, A e B. Se A tiver 200 CPU Shares e B tiver 100 CPU Shares, o CFS (scheduler) tentar√° alocar o dobro do tempo de CPU para A em rela√ß√£o a B, quando ambos estiverem competindo por recursos.|
|**CFS (Completely Fair Scheduler)**|O CFS garante um escalonamento justo de tempo de CPU entre os processos, levando em considera√ß√£o as prioridades definidas pelas CPU Shares atribu√≠das a cada Cgroup. Processos dentro de Cgroups com mais shares ter√£o acesso proporcionalmente maior √† CPU.| A compreens√£o do funcionamento do CFS √© importante para otimizar a performance dos pods, especialmente em ambientes com alta concorr√™ncia por recursos.|

### Aloca√ß√£o vs. Consumo Real

A aloca√ß√£o incorreta de recursos pode impedir o agendamento de pods ou levar a desperd√≠cios. Quando os requests s√£o muito altos em compara√ß√£o com o consumo real, os recursos acabam ficando reservados e s√£o subutilizados, enquanto outros pods que precisam desses recursos ficam impedidos de serem agendados.

* **Impactos pro Banco:**
  * custos desnecess√°rios;
  * desperd√≠cio de recursos;
  * superdimensionamento;
  * gerenciamento complicado;
  * dificuldade em identificar e resolver problemas de desempenho.
  <br>
* **Impactos pro cliente:** 
  * desempenho inconsistente - lat√™ncia e disponibilidade;
  * qualidade do servi√ßo;
  * experi√™ncia do usu√°rio.
  
### Calculando recursos para um cont√™iner

Para estimar a necessidade real do cont√™iner, muitos fatores devem ser considerados. Podemos destacar:
* **Recursos do Sistema Operacional:** o sistema operacional no n√≥ consome CPU e mem√≥ria;
* **Componentes de Gerenciamento do Kubernetes:** Kubelet, runtime do cont√™iner (Docker, containerd) e agentes de rede tamb√©m consomem recursos;
* **Overhead do Pod:** inclui sobrecarga de gerenciamento e execu√ß√£o de processos auxiliares;
* **Escalabilidade e picos de demanda**;
* **R√©plicas de cont√™ineres para suportar falhas de n√≥**;
* **Estrat√©gias de recupera√ß√£o autom√°tica**;
* **Atualiza√ß√µes de software**.
   
## Caso de uso

Voc√™ est√° implantando um servi√ßo em um cluster Kubernetes e precisa configurar os recursos de CPU e mem√≥ria para garantir que seu cont√™iner A tenha uma aloca√ß√£o adequada, respeitando os recursos dispon√≠veis para todos os componentes.

Primeiro, voc√™ precisa descobrir os limites (**quota Hard**) definidos para recursos dentro do namespace, como CPU e mem√≥ria. Para isso, utilize o comando `kubectl describe namespace NOME_NAMESPACE`. Este comando pode ser sempre utilizado para consultar e comparar os recursos usados (**quota Used**) e a capacidade m√°xima do namespace.

Ap√≥s descobrir o total permitido, voc√™ ir√° estimar o quanto de recursos ser√° consumido por componentes como o kubelet, overhead do pod, etc. para descobrir o quanto de recursos voc√™ tem de fato, dispon√≠veis para alocar.

### Diagrama da hierarquia
```
Cluster (Total: 120 CPUs, 200 GiB de Mem√≥ria)
‚îú‚îÄ‚îÄ Namespace A
‚îÇ   ‚îú‚îÄ‚îÄ Pod A
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Cont√™iner A
‚îú‚îÄ‚îÄ N√≥ 1 (24 CPUs, 40 GiB de Mem√≥ria)
‚îÇ   ‚îú‚îÄ‚îÄ Pod A (Namespace A)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Cont√™iner A
‚îÇ   ‚îî‚îÄ‚îÄ Outros componentes do n√≥: kubelet, kube-proxy, etc. (2 CPUs, 4 GiB de Mem√≥ria)
‚îú‚îÄ‚îÄ N√≥ 2 (24 CPUs, 40 GiB de Mem√≥ria)
‚îú‚îÄ‚îÄ N√≥ 3 (24 CPUs, 40 GiB de Mem√≥ria)
‚îú‚îÄ‚îÄ N√≥ 4 (24 CPUs, 40 GiB de Mem√≥ria)
‚îî‚îÄ‚îÄ N√≥ 5 (24 CPUs, 40 GiB de Mem√≥ria)
```

Baseado nos valores colocados acima, voc√™ subtraiu os valores de consumo das fun√ß√µes b√°sicas do cont√™iner do valor de capacidade total do n√≥ e chegou a conclus√£o que ainda possui 22 CPUs e 36 GiB dispon√≠veis de recursos para alocar no cont√™iner A. 

> :warning: **Aten√ß√£o** 
> 
> Ter recursos dispon√≠veis n√£o significa que voc√™ deve utilizar a capacidade m√°xima permitida. Ao configurar esses valores, use o bom senso para assegurar efici√™ncia e disponibilidade. Um n√∫mero excessivo de requests pode resultar em subutiliza√ß√£o, enquanto um n√∫mero insuficiente pode afetar o desempenho. Monitore o consumo real e ajuste conforme necess√°rio.

### Exemplo de configura√ß√£o do YAML

```yaml
      resources:
        requests:
          cpu: "4000m"    ## Solicita√ß√£o de 4 CPUs (incluindo 2 para recursos b√°sicos e 2 aloc√°veis adicionais), garante que n√£o ocorrer√° eviction
          memory: "12000Mi"  # 12 GiB = 4 GiB (recursos b√°sicos) + 8 GiB (aloc√°veis adicionais), garante que n√£o ocorrer√° eviction
        limits:
          cpu: "24000m"   # Limite de 24 CPUs (6 vezes maior que o request de 4 CPUs), essa sugest√£o evita a ocorr√™ncia de throttling
          memory: "24000Mi" 
```

> :information_source: **Observa√ß√£o** 
> 
> Note que, mesmo tendo a sua disposi√ß√£o 40 GiB de mem√≥ria, o arquivo n√£o foi configurado para consumir o recurso na totalidade.
> 
#### C√°lculo do CFS

No exemplo acima, configuramos o request de CPU para 4000 milicores (4 CPUs) e o limit para 24000 milicores (24 CPUs). O CFS, que distribui o tempo de CPU com base em shares, calcula esses valores assumindo 1024 shares por CPU. Assim, 6 CPUs correspondem a 4096 shares. 

Se houver concorr√™ncia por CPU entre m√∫ltiplos pods do nosso exemplo, o CFS distribuir√° o tempo de CPU proporcionalmente a esses shares. Se tivermos um pod B, com 3072 shares, o CFS alocar√° 2/3 do tempo de CPU para o Pod A e 1/3 para o Pod B.

**Tags:** #kubernetes #recursos #microsservicos

## A Seguir
* Leia o roteiro [Configura√ß√£o de Recursos em Microservi√ßos](https://fontes.intranet.bb.com.br/psc/publico/atendimento/-/wikis/Roteiros/ICE/rtc) para aprender a ajustar a Quota do Namespace e de Limit Range.
* Acesse a wiki [Conceitos de Gest√£o de Capacidade e Efici√™ncia](https://fontes.intranet.bb.com.br/psc/publico/atendimento/-/wikis/Roteiros/ICE/main) para conhecer mais sobre os conceitos relacionados √† Gest√£o de Capacidade e Efici√™ncia na BB Cloud.

## Precisa de ajuda?
Em caso de problemas na execu√ß√£o de qualquer roteiro, abra uma [nova issue](https://fontes.intranet.bb.com.br/dev/publico/atendimento/-/issues) e relate a situa√ß√£o. O time de atendimento da devCloud est√° dispon√≠vel para auxiliar!  

## Este roteiro foi √∫til?
[^1]: [üëçüëé](http://feedback.dev.intranet.bb.com.br/?origem=roteiros&url_origem=fontes.intranet.bb.com.br/dev/publico/roteiros/-/blob/master/kubernetes/referencias/alocar_recursos_microsservicos.md&internalidade=kubernetes/referencias/alocar_recursos_microsservicos)
